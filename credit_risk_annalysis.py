# -*- coding: utf-8 -*-
"""Credit_risk_Annalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ouofgMveow36g171tIxAjjkSotTUDdzY
"""

import pandas as pd
import numpy as np

credit_risk_df = pd.read_csv('/content/drive/MyDrive/DeepLearning/credit_risk_dataset.csv')

cr_df = credit_risk_df.copy()

cr_df.head()

dups= cr_df.duplicated()

cr_df[dups].head()

cr_df.query("person_age==23 & person_income==42000 & person_home_ownership=='RENT' & loan_int_rate==9.99")

cr_df.shape

cr_df.drop_duplicates(inplace=True)

cr_df.shape

from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV 
x_train,x_test,y_train, y_test = train_test_split(cr_df.drop('loan_status',axis=1), cr_df['loan_status'], random_state=0,test_size=0.2,stratify= cr_df['loan_status'],shuffle=True)

cr_df['loan_status'].value_counts()

cr_df['loan_status'].value_counts(normalize=True)

y_train.value_counts(normalize=True)

y_test.value_counts(normalize=True)

np.round(x_train.isnull().sum() *100/x_train.shape[0],2)

cr_df['person_emp_length'].unique()

from sklearn.impute import SimpleImputer
impute = SimpleImputer(strategy='median')
data_array = impute.fit_transform(cr_df)
cr_df1=pd.DataFrame(data_array, columns=cr_df.columns)

x_train.shape

x_train.dropna().shape

"""# Removed 'NULL' values i.e 12% of data removed"""

x_train[['person_income','loan_amnt','loan_percent_income']].head()

x_train.drop('loan_percent_income', axis=1, inplace= True)

x_test.drop('loan_percent_income', axis=1, inplace= True)

x_train.head()

"""#Count the unique values columnwise using 'nunique'
# if the column has any categirical values(check nuniques is <20) then count using " value count
"""

for col in x_train:
  print(col,'----->',x_train[col].nunique())
  if x_train[col].nunique()<20:
    print(x_train[col].value_counts(normalize=True)*100)

x_train.describe()

"""# Observation
## Person age Max = 144 (outlier)
## person_emp_length(service year) = 123 ( outlier)
"""

x_train.head(2)

"""#Select only Numeric Columns
## Age, Person_income, person_emp_length,loan_amnt,loan_int_rate,#cb_person_cred_hist_length
"""

for col in x_train:
  if x_train[col].dtype !='object':
    num_cols = col
    print(num_cols)

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

for col in num_cols:
   sns.histplot(x_train[col])
   plt.show()

x_train.loc[x_train['person_age']>=80, :]

x=x_train.loc[x_train['person_age']<80, :]

x.shape

x.loc[x['person_emp_length']>=66,:]

cr_df.query("person_age<=person_emp_length+14")

x=x.loc[(x['person_emp_length']<66) | (x['person_emp_length'].isna()), :]

"""# Index of x will be pass to Y since some rows are removed from x(train)"""

y= y_train[x.index]

category_columns = [col for col in x if x[col].dtype == 'O']
category_columns

"""# Pipeline for handling missing values"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import plot_precision_recall_curve
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import plot_confusion_matrix, confusion_matrix,classification_report
from lightgbm import LGBMClassifier

num_pipe = Pipeline([
    ('impute', IterativeImputer()),
    ('Scale', StandardScaler()),
])

"""## Apply Impute and Scaler to handle missing values in the Numeric coumns(Num_cols)
## Apply one hot encoder to transfer catogorical values(cat_cols)
"""

ct = ColumnTransformer([
    ('num_pipe', num_pipe, num_cols),
    ('cat_cols', OneHotEncoder(sparse = False,handle_unknown='ignore'), category_columns)
], remainder = 'passthrough')

#grid is a dictionary which includes different models 
grid = {
    RandomForestClassifier(random_state=0, n_jobs=-1, class_weight='balanced'):
    {'model_n_estimators':[300,400,500],
     'colf_num_pipe_impute_estimator':[LinearRegression(), RandomForestRegressor(random_state=0), KNeighborsRegressor()]},

    LGBMClassifier(random_state=0, n_jobs=-1, class_weight='balanced'):
    {'model_n_estimators':[300,400,500],
     'model_learning_rate':[0.001, 0.01, 0.1,1,10],
     'model_boosting_type':['gbdt','goss','dart'],
     'colf_num_pipe_impute_estimator':[LinearRegression(), RandomForestRegressor(random_state=0), KNeighborsRegressor()]},
}

for clf, param in grid.items():
  print(clf)
  print('-'*50)
  print(param)
  print('\n')

full_df = pd.DataFrame()
best_algos = {}

for clf, param in grid.items():
  pipe = Pipeline([
      ('coltf', ct),
      ('model',clf)
])
  gs = RandomizedSearchCV(estimator=pipe, param_distributions=param,scoring='accuracy', 
                          n_jobs=-1, verbose=3, n_iter=4, random_state=0)
  gs.fit(x,y)
  all_res = pd.DataFrame(gs.cv_results_)

  temp = all_res.loc[:, ['param','mean_test_score']]
  algo_name = str(clf).split('(')[0]    # substring before '(' of algorthm name
  temp['algo'] = algo_name

  full_df = pd.concat([full_df, temp], ignore_index=True)
  best_algos[algo_name] = gs.best_estimator_

full_df.sort_values('mean_test_score',ascending=False)

be = best_algos['RandomForestClassifier']
bbe

be.fit(x,y)

preds = be.predict(x_test)

confusion_matrix(y-Test, preds)

plot_confusion_matrix(be, x_test,y_test)

print(classification_report(y_test, preds))

be.score(x_test, y_test)

"""#Precision recall curve"""

plot_precision_recall_curve(estimator=be, x=x_test, y=y_test, name = 'model AUC')
baseline = y_test.sum()/ len(y_test)
plt.axhline(baseline, ls ='--', color='r', label=f'Baseline model ({round(baseline,2)})')
plt.legend(loc='best')

a,b,c = learning_curve(be, x, y, n_jobs=-1, scoring='accuracy')

plt.plot(a,b.mean(axis=1), label = 'traing accuracy')
plt.plot(a,c.mean(axis=1), label = 'validation accuracy')
plt.ylabel('accuracy')
plt.xlabel('Training sample size')
plt.legend()

"""#Overfitting since gap is too much

1. High trainng accuracy ( low bias) 
2. Low testing or validation accuracy(high variance)
3. Big gap between training and validation curves( high variance)
4. Overfitting makes a very complex model and learns even the ' nose' in the data, which is undesirable

# Remidial for Overfitting
1. Add more training samples, if possible to allow the model to learn better
2. Working with data at hand:
      Make a simpler model/ reduce complexity of model:
      * try reducing number of featuress
      * try incresing regularization (lambda)
      * try pruning the decision trees
"""

grid = {
    RandomForestClassifier(random_state=0, n_jobs=-1, class_weight='balanced'):
    {'model_n_estimators':[100,200,300],
     'model_max_depth':[5,9,13],
     'model_min_samples_split':[4,6,8],
     'colf_num_pipe_impute_estimator':[LinearRegression(), RandomForestRegressor(random_state=0), KNeighborsRegressor()]},

   # LGBMClassifier(random_state=0, n_jobs=-1, class_weight='balanced'):
   # {'model_n_estimators':[300,400,500],
   #  'model_learning_rate':[0.001, 0.01, 0.1,1,10],
   #  'model_boosting_type':['gbdt','goss','dart'],
   #  'colf_num_pipe_impute_estimator':[LinearRegression(), RandomForestRegressor(random_state=0), KNeighborsRegressor()]},
}

for clf, param in grid.items():
  print(clf)
  print('-'*50)
  print(param)
  print('\n')

full_df = pd.DataFrame()
best_algos = {}

for clf, param in grid.items():
  pipe = Pipeline([
      ('coltf', ct),
      ('model',clf)
])
  gs = RandomizedSearchCV(estimator=pipe, param_distributions=param,scoring='accuracy', 
                          n_jobs=-1, verbose=3, n_iter=4, random_state=0)
  gs.fit(x,y)
  all_res = pd.DataFrame(gs.cv_results_)

  temp = all_res.loc[:, ['param','mean_test_score']]
  algo_name = str(clf).split('(')[0]    # substring before '(' of algorthm name
  temp['algo'] = algo_name

  full_df = pd.concat([full_df, temp], ignore_index=True)
  best_algos[algo_name] = gs.best_estimator_



"""# Contunie All the steps
Please note:
## if recall value is HIGH, the model is good.
# recude gap betwenn train and test data accuracy
# either try parameter optimization or try different algorithm
"""



